{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucL1cskVylk0"
      },
      "outputs": [],
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUf595PGzGwU"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "import random\n",
        "\n",
        "okt = Okt()\n",
        "okt.pos('우리 디자인 교육관을 가지 않고 사회교육관으로 가고 싶다')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt.morphs(\"나는 한국대학교 자연캠퍼스에 갈거야\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_5ZxEBuVs2D",
        "outputId": "f0484c6c-7e83-4bdc-8390-70d0ca070f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['나', '는', '한국', '대학교', '자연', '캠퍼스', '에', '갈거야']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kPm38kuzVgC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "f = open(\"./validation_set.txt\", \"w\")\n",
        "entities = []\n",
        "#library.append(library_added)\n",
        "\n",
        "i = 1\n",
        "\n",
        "\n",
        "def tagging(sentence):\n",
        "\n",
        "  okt = Okt()\n",
        "  words = okt.morphs(sentence)\n",
        "  flag = 0\n",
        "  preprocess_word = []\n",
        "  for word in words:\n",
        "    if '을' in word or '를' in word or'로' in word or '으로' in word or '에' in word:\n",
        "      for char in word:\n",
        "        preprocess_word.append(char)\n",
        "    else :\n",
        "      preprocess_word.append(word)\n",
        "\n",
        "  tagged_result = []\n",
        "\n",
        "  for word in preprocess_word:\n",
        "      if (word in library) and flag == 0:\n",
        "        tagged_result.append((word, 'B_LOC_pos'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\" + word + \"\\t\" +\"B_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"B_LOC_pos\" + \"\\n\")\n",
        "        flag = 1\n",
        "\n",
        "      elif word in library:\n",
        "        tagged_result.append((word, 'I_LOC_pos'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"I_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"I_LOC_pos\" + \"\\n\")\n",
        "\n",
        "      else:\n",
        "        tagged_result.append((word, '0'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\"  +\"O\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"O\"+ \"\\n\")\n",
        "\n",
        "\n",
        "  print(\"\\n\")\n",
        "  f.write(\"\\n\\n\")\n",
        "  return tagged_result\n",
        "\n",
        "# sentence = \"우리 디자인 교육관를 가지 않고 사회교육관으로 가고 싶다\"\n",
        "\n",
        "# result = tagging(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRvTlHdMm0dX"
      },
      "outputs": [],
      "source": [
        "def tagging_1(sentence):\n",
        "\n",
        "  okt = Okt()\n",
        "  words = okt.morphs(sentence)\n",
        "  flag = 0\n",
        "  preprocess_word = []\n",
        "\n",
        "  for word in words:\n",
        "    if '을' in word or '를' in word or '로' in word or '으로' in word or '에' in word:\n",
        "      for char in word:\n",
        "        preprocess_word.append(char)\n",
        "    else :\n",
        "      preprocess_word.append(word)\n",
        "  tagged_result = []\n",
        "  for word in preprocess_word:\n",
        "      if (word in library) and flag == 0:\n",
        "        tagged_result.append((word, 'B_LOC'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\" + word + \"\\t\" +\"B_LOC_neg\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"B_LOC_neg\" + \"\\n\")\n",
        "        flag = 1\n",
        "\n",
        "      elif (word in library) and flag == 1:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"I_LOC_neg\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"I_LOC_neg\" + \"\\n\")\n",
        "\n",
        "      elif (word in library) and flag == 2:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "            #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"B_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"B_LOC_pos\" + \"\\n\")\n",
        "        flag = 3\n",
        "\n",
        "      elif (word in library) and flag ==3:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "            #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"I_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"I_LOC_pos\" + \"\\n\")\n",
        "\n",
        "      elif flag == 1 or flag == 2:\n",
        "        tagged_result.append((word, '0'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\"  +\"O\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"O\"+ \"\\n\")\n",
        "        flag = 2\n",
        "\n",
        "\n",
        "      else:\n",
        "        tagged_result.append((word, '0'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\"  +\"O\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"O\"+ \"\\n\")\n",
        "        flag = 0\n",
        "  print(\"\\n\")\n",
        "  f.write(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZz4Er9qzqUR"
      },
      "outputs": [],
      "source": [
        "person = ['','나는', '내가', '나', '우리는', '우리'] # 6개\n",
        "\n",
        "\n",
        "sentence_pos = [\"\",\"에 가고 싶어\" , \"에 갈거야\", \"에 갈 예정이야\", \" 갈래\", \"에 갈래\", \"에 가고 싶습니다\", \" 가고 싶습니다\", \" 가고 싶어요\", \"에 가고 싶어요\",\" 가도 되나요\", \" 가도 될까요\",\"을 가도 될까요\", \" 가게 해줘\", \"으로 가고 싶다\", \"으로 가고 싶습니다\"] # 16개\n",
        "sentence_neg_1 = [\" 말고 \", \" 가지 않고 \", \" 경유하지 않고 \", \" 안 가고 \", \" 피해서 \", \" 가지 않고 \" ,\"을 가지 않고 \"]\n",
        "sentence_neg_2 = [\" 안 갈거야\", \" 안갈거야\", \" 피할거야\",\"는 경유하지 않을겁니다\",\" 경유하지 않을거야\", \" 지나치지 않을거야\", \"을 지나치지 않을겁니다\", \" 경유하지 않을겁니다\"]\n",
        "i = 1\n",
        "j = 0\n",
        "\n",
        "\n",
        "\n",
        "for _ in person:\n",
        "  for _ in places:\n",
        "    for _ in sentence_pos:\n",
        "      subject = random.choice(person)\n",
        "      object = random.choice(places)\n",
        "      verb = random.choice(sentence_pos)\n",
        "      string = subject + \" \" + object + verb\n",
        "      result = tagging(string)\n",
        "      i = i + 1\n",
        "          #print(result)\n",
        "      with open(\"./dataset.json\", \"w\", encoding = \"utf-8\") as json_file:\n",
        "        json.dump(result, json_file, ensure_ascii=False, )\n",
        "\n",
        "      if (i == 1000):\n",
        "        break\n",
        "    if(i == 1000):\n",
        "      break\n",
        "  if(i == 1000):\n",
        "      break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzvQqeD0ogC6"
      },
      "outputs": [],
      "source": [
        "for _ in person:\n",
        "  for _ in range(0,10) :\n",
        "    for _ in sentence_neg_1:\n",
        "      for _ in range(0,10):\n",
        "        for _ in sentence_pos:\n",
        "          object_1 = random.choice(places)\n",
        "          object_2 = random.choice(places)\n",
        "\n",
        "          subject = random.choice(person)\n",
        "          verb_1 = random.choice(sentence_neg_1)\n",
        "          verb_2 = random.choice(sentence_pos)\n",
        "          string = subject + \" \" + object_1 + verb_1 + object_2 + verb_2\n",
        "          result = tagging_1(string)\n",
        "          i = i + 1\n",
        "            #print(result)\n",
        "          with open(\"./dataset.json\", \"w\", encoding = \"utf-8\") as json_file:\n",
        "            json.dump(result, json_file, ensure_ascii=False, )\n",
        "          if (i == 6000):\n",
        "            break\n",
        "        if (i == 6000):\n",
        "            break\n",
        "      if (i == 6000):\n",
        "            break\n",
        "    if (i == 6000):\n",
        "            break\n",
        "  if (i == 6000):\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIyO3tT78VPu"
      },
      "outputs": [],
      "source": [
        "def tagging_3(sentence):\n",
        "\n",
        "  okt = Okt()\n",
        "  words = okt.morphs(sentence)\n",
        "  flag = 0\n",
        "  preprocess_word = []\n",
        "\n",
        "  for word in words:\n",
        "    if '을' in word or '를' in word or '로' in word or '으로' in word or '에' in word:\n",
        "      for char in word:\n",
        "        preprocess_word.append(char)\n",
        "    else :\n",
        "      preprocess_word.append(word)\n",
        "  tagged_result = []\n",
        "  for word in preprocess_word:\n",
        "      if (word in library) and flag == 0:\n",
        "        tagged_result.append((word, 'B_LOC'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\" + word + \"\\t\" +\"B_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"B_LOC_pos\" + \"\\n\")\n",
        "        flag = 1\n",
        "\n",
        "      elif (word in library) and flag == 1:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"I_LOC_pos\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"I_LOC_pos\" + \"\\n\")\n",
        "\n",
        "      elif (word in library) and flag == 2:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "            #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"B_LOC_neg\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"B_LOC_neg\" + \"\\n\")\n",
        "        flag = 3\n",
        "\n",
        "      elif (word in library) and flag ==3:\n",
        "        tagged_result.append((word, 'I_LOC'))\n",
        "            #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\" +\"I_LOC_neg\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" +\"I_LOC_neg\" + \"\\n\")\n",
        "\n",
        "      elif flag == 1 or flag == 2:\n",
        "        tagged_result.append((word, '0'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\"  +\"O\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"O\"+ \"\\n\")\n",
        "        flag = 2\n",
        "\n",
        "\n",
        "      else:\n",
        "        tagged_result.append((word, '0'))\n",
        "        #entities.append(word)\n",
        "        print( str(i) + \"\\t\"  + word + \"\\t\"  +\"O\")\n",
        "        f.write(str(i) + \"\\t\" + word + \"\\t\" + \"O\"+ \"\\n\")\n",
        "        flag = 0\n",
        "  print(\"\\n\")\n",
        "  f.write(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_fn0NtQm_yT"
      },
      "outputs": [],
      "source": [
        "for _ in person:\n",
        "  for _ in range(0,5):\n",
        "    for _ in sentence_pos:\n",
        "      for _ in range(0,5):\n",
        "        for _ in sentence_neg_2:\n",
        "          object_1 = random.choice(places)\n",
        "          object_2 = random.choice(places)\n",
        "\n",
        "          subject = random.choice(person)\n",
        "          verb_1 = random.choice(sentence_pos)\n",
        "          verb_2 = random.choice(sentence_neg_2)\n",
        "\n",
        "          string = subject + \" \" + object_1 + verb_1 + object_2 + verb_2\n",
        "          result = tagging_3(string)\n",
        "          i = i + 1\n",
        "            #print(result)\n",
        "          with open(\"./dataset.json\", \"w\", encoding = \"utf-8\") as json_file:\n",
        "            json.dump(result, json_file, ensure_ascii=False, )\n",
        "          if (i == 10000):\n",
        "            break\n",
        "        if (i == 10000):\n",
        "            break\n",
        "      if (i == 10000):\n",
        "            break\n",
        "    if (i == 10000):\n",
        "            break\n",
        "  if (i == 10000):\n",
        "    break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}